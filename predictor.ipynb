{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_data.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.axhline(0)\n",
    "for i in [-0.15, -0.1, -0.05, 0.05, 0.1, 0.15]:\n",
    "    plt.axhline(i, color = \"black\", linestyle=':', linewidth = 1)\n",
    "\n",
    "plt.xlabel(\"Predictor\")\n",
    "plt.ylabel(\"Corr(Predictor, score)\")\n",
    "bars = bars = df.corr()['score'][df.corr()['score'].abs().sort_values(ascending=False).index][range(1,11)].plot(kind='bar')\n",
    "plt.savefig('corr_dist.png', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_predictors(df):\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    predictors = abs(df.corr()['score']).sort_values(ascending=False).drop(\"score\")\n",
    "\n",
    "    r2_max = 0\n",
    "    i_max = 0\n",
    "\n",
    "    # add variables until model is overfitting\n",
    "    for i, col in enumerate(list(predictors.index)):\n",
    "        X = df[predictors.index[:i+1]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, df[\"score\"], test_size=0.3, random_state=42)\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        if r2 > r2_max:\n",
    "            r2_max = r2\n",
    "            i_max = i+1\n",
    "\n",
    "    # return predictors\n",
    "    return predictors[0:i_max] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train again best model\n",
    "predictors  = find_predictors(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[list(predictors.index)], df[\"score\"], test_size=0.3, random_state=42)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "len(predictors), round(r2, 5), round(MSE, 5), round(explained_var,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlation between independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.iloc[:,0:-1].corr()\n",
    "high_corr1, high_corr2 = np.where(np.abs(corr)>0.5)\n",
    "# get indicics of highly correlated features in feature matrix\n",
    "A = []\n",
    "for i in range(len(high_corr1)):\n",
    "    if high_corr1[i]!=high_corr2[i]:\n",
    "        A.append((high_corr1[i],high_corr2[i]))\n",
    "        \n",
    "# transform indices to names of columns, add to tuple together with correlation of the features\n",
    "B = []\n",
    "for i in range(len(A)):\n",
    "    B.append((df.columns[A[i][0]],df.columns[A[i][1]], round(corr.iloc[A[i][0],A[i][1]],3)))\n",
    "    \n",
    "# transform indices to names of columns, add to tuple together with correlation of the features\n",
    "B = []\n",
    "for i in range(len(A)):\n",
    "    B.append((df.columns[A[i][0]],df.columns[A[i][1]], round(corr.iloc[A[i][0],A[i][1]],3)))\n",
    "B\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms conditioned on MMO, Indie and release_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,1,21)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(16, 5))\n",
    "fig.suptitle('MMO', fontsize=16)\n",
    "ax1.hist(df.loc[df['MMO'] == 1][\"score\"],bins)\n",
    "ax2.hist(df.loc[df[\"MMO\"] == 0][\"score\"],bins)\n",
    "ax1.set_title(\"MMO=1\")\n",
    "ax2.set_title(\"MMO=0\")\n",
    "ax3.hist(df.loc[df['MMO'] == 1][\"score\"],bins, density=True, alpha=0.5)\n",
    "ax3.hist(df.loc[df[\"MMO\"] == 0][\"score\"],bins, density=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confirm with Whitney U-Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(df.loc[df[\"MMO\"] == 0][\"score\"], df.loc[df['MMO'] == 1][\"score\"],alternative=\"less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whitneyu provides evidence that distribution of MMO=1 is NOT higher than distribution of MMO=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,1,21)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(16, 5))\n",
    "fig.suptitle('Downloadable Content', fontsize=16)\n",
    "ax1.hist(df.loc[df['Downloadable Content'] == 1][\"score\"],bins)\n",
    "ax2.hist(df.loc[df['Downloadable Content'] == 0][\"score\"],bins)\n",
    "ax1.set_title(\"Downloadable Content=1\")\n",
    "ax2.set_title(\"Downloadable Content=0\")\n",
    "ax3.hist(df.loc[df['Downloadable Content'] == 1][\"score\"],bins, density=True, alpha=0.5)\n",
    "ax3.hist(df.loc[df['Downloadable Content'] == 0][\"score\"],bins, density=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,1,21)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(16, 5))\n",
    "fig.suptitle('Downloadable Content', fontsize=16)\n",
    "ax1.hist(df.loc[df['Indie'] == 1][\"score\"],bins)\n",
    "ax2.hist(df.loc[df['Indie'] == 0][\"score\"],bins)\n",
    "ax1.set_title('Indie=1')\n",
    "ax2.set_title('Indie=0')\n",
    "ax3.hist(df.loc[df['Indie'] == 1][\"score\"],bins, density=True, alpha=0.5)\n",
    "ax3.hist(df.loc[df['Indie'] == 0][\"score\"],bins, density=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(df.loc[df['Indie'] == 0][\"score\"], df.loc[df['Indie'] == 1][\"score\"], alternative=\"less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitneyu provides evidence that Indie games are better than non-indie games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(len(set(df[\"relase_year\"])))\n",
    "for i in range(len(set(df[\"relase_year\"]))):\n",
    "    A[i] = np.mean(df.loc[lambda df: df.relase_year == 1998+i,\"score\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A,\".\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"release year\")\n",
    "plt.xticks([2,7,12,17,22],labels=[\"2000\",\"2005\", \"2010\", \"2015\", \"2020\"])\n",
    "plt.title(\"average game-score by release year\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the looks of it, games seem to have been better (rated) in the past. However, the plot is to be enjoyed with caution,\n",
    "e.g. for the years 1998,1999 are only 1-2 games in the data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data before using it as NN input\n",
    "df_stand = df.copy(deep=True)\n",
    "age = df[\"required_age\"]\n",
    "df_stand[\"required_age\"] = (age-np.mean(age))/np.std(age)\n",
    "price = np.array(df[\"price\"]).reshape(-1)\n",
    "df_stand[\"price\"] = (price-np.mean(price))/np.std(price)\n",
    "year = df[\"relase_year\"]\n",
    "year = df[\"relase_year\"]\n",
    "df_stand[\"relase_year\"] = (year-np.mean(year))/np.std(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamData(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.data_size = len(self.df)\n",
    "        self.num_variables = len(self.df.columns[0:-1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        IVs = torch.from_numpy(np.array(self.df.iloc[idx,0:-1],dtype=np.double))\n",
    "        target = torch.from_numpy(np.array(self.df.loc[idx,\"score\"], dtype=np.double))\n",
    "        \n",
    "        sample = (IVs,target)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloader_steam = SteamData(df_stand)\n",
    "\n",
    "BATCHSIZE = 64\n",
    "\n",
    "generator=torch.Generator().manual_seed(42)\n",
    "\n",
    "train_size = int(0.7 * len(df_stand))\n",
    "test_size = len(df_stand) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(Dataloader_steam, [train_size, test_size], generator =generator)\n",
    "\n",
    "SteamData_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCHSIZE,\n",
    "shuffle=True)\n",
    "\n",
    "SteamData_val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCHSIZE,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(53,128)\n",
    "        self.lin2 = nn.Linear(128,256)\n",
    "        self.lin3 = nn.Linear(256,128)\n",
    "        self.lin4 = nn.Linear(128,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = F.leaky_relu(self.lin1(x))\n",
    "        out = F.leaky_relu(self.lin2(out))\n",
    "        out = F.leaky_relu(self.lin3(out))\n",
    "        out = torch.sigmoid(self.lin4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN()\n",
    "model = model.to(device=device)\n",
    "model = model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "model.train()\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
    "    loss_train = np.array(n_epochs)\n",
    "    loss_val = np.array(n_epochs)\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss_train = 0\n",
    "        total_loss_val = 0\n",
    "        for IV, score in train_loader:\n",
    "            IV = IV.to(device=device)\n",
    "            score = score.to(device=device)\n",
    "            outputs = model(IV)\n",
    "            loss_train = loss_fn(outputs.flatten(), score)\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_train+=loss_train\n",
    "        \n",
    "        for IV, score in val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                IV = IV.to(device=device)\n",
    "                score = score.to(device=device)\n",
    "                outputs = model(IV)\n",
    "                loss_val = loss_fn(outputs.flatten(), score)\n",
    "                \n",
    "                total_loss_val+=loss_val\n",
    "       \n",
    "            \n",
    "            \n",
    "\n",
    "        if epoch ==1 or epoch % 1 == 0:\n",
    "            print(\"{} Epoch {}, Training loss {}, Validation Loss: {}\".format(\n",
    "            datetime.datetime.now(), epoch,\n",
    "            total_loss_train/ len(train_loader), total_loss_val/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(n_epochs, optimizer, model, loss_fn, SteamData_train_loader, SteamData_val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check testset with NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = np.zeros((len(SteamData_val_loader.dataset),53))\n",
    "test_samples_score = np.zeros(len(SteamData_val_loader.dataset))\n",
    "idx=0\n",
    "for IV,score in SteamData_val_loader:\n",
    "    test_samples[idx:idx+IV.shape[0]] = IV\n",
    "    test_samples_score[idx:idx+IV.shape[0]] = score\n",
    "    idx=idx+IV.shape[0]\n",
    "test_samples = (torch.from_numpy(test_samples)).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred_NN = model(test_samples)\n",
    "\n",
    "explained_variance_score(test_samples_score,y_pred_NN.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between regression and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[best_fit_indices_min[1]]],df[\"score\"] , test_size=0.3, random_state=42)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_pred, y_test)\n",
    "explained_var, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.4,1,21)\n",
    "bins2 = np.linspace(-0.5,0.75,30)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16, 5))\n",
    "ax1.hist(y_pred, bins, alpha=0.5, label=\"linear regression\")\n",
    "ax1.hist(y_pred_NN.cpu().numpy().reshape(-1), bins, alpha=0.5, label=\"NN\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"predicted scores\")\n",
    "ax1.set_xlabel(\"score\")\n",
    "ax2.hist(y_pred-y_test, bins2, alpha=0.5, label=\"linear regression\")\n",
    "ax2.hist(y_pred_NN.cpu().numpy().reshape(-1)- test_samples_score, bins2, alpha=0.5, label=\"NN\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"residuals of predictions\")\n",
    "ax2.set_xlabel(\"residuals/erros\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0e9a04e0a5c422eb7d17f089ac60eed9f8e8db0d5bb014daab27a4ac46018303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
